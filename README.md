# VBMC Review and its correlation with Policy Gradient based RL algorithm.
This work/project was done as part of a course project. The course's link: [TPMI](https://www.cse.iitk.ac.in/users/piyush/courses/tpmi_winter21/tpmi.html).
Variational Bayesian Monte Carlo and Noisy Likelihoods: A Review and some new suggestions : This report is a review of the paper Variational Bayesian Monte Carlo by Luigi Acerbi and of further extension of VBMC([1](https://github.com/lacerbi/vbmc), [2](https://github.com/lacerbi/infbench#references)) that addresses the limitations of the former paper by including noisy likelihoods. VBMC is an approximate inference method that combines Variational Inference with Bayesian Quadrature. We perform experiments as done in the original paper, compare with the benchmarks and provide our insights regarding them.

We find a very close correlation between VBMC and the routine procedure of Policy Gradient based RL algorithms. And looking at this correlation and the performance improvement of more Policy Gradient methods that use Natural Gradient ascent/descent instead of vanila Gradient ascent/descent, we suggest the same change in the VBMC algorithm. For more information, kindly refer to the report(Our_Awesome_Probabilistic_Modeling_Project.pdf) and the presentation (TPMI_Presentation.pdf). With this correlation, we also try to point to some theoretical gurantees about VBMC that is somewhat missing from the original paper.
![alt text](https://github.com/gmadaan15/VBMC_review/blob/main/Correlation_2.png)
